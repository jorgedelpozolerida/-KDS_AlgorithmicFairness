{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability and Ethics\n",
    "## Assignment 2\n",
    "Kamil Kojs, Janos Mate, Jorge del Pozo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, \\\n",
    "                                confusion_matrix,  accuracy_score, precision_score, \\\n",
    "                                    recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import scipy\n",
    "from scipy.optimize import fmin_tnc\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*).\n",
    "\n",
    "1. As last week, we are still predicting the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`).\n",
    "2. Today, we are going to implement two methods for data debiasing: [Fair PCA](https://deepai.org/publication/efficient-fair-pca-for-fair-representation-learning) and [A Geometric Solution to Fair Representations](https://dl.acm.org/doi/10.1145/3375627.3375864).\n",
    "3. We are going to evaluate the performance on two sensitive features: `SEX` (i.e. *Males* and *Females*) and `RAC1P` (we will consider only *Whites* and *African-Americans*)\n",
    "4. I updated the filtering method `adult_filter` to keep the specified groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(\n",
    "    survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "mm_scalar = MinMaxScaler()\n",
    "\n",
    "\n",
    "def adult_filter(data):\n",
    "    \"\"\"Mimic the filters in place for Adult data.\n",
    "    Adult documentation notes: Extraction was done by Barry Becker from\n",
    "    the 1994 Census database. A set of reasonably clean records was extracted\n",
    "    using the following conditions:\n",
    "    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    df = df[df[\"RAC1P\"] < 3]  # keep only Whites and African-Americans\n",
    "    return df\n",
    "\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP',\n",
    "        'COW',\n",
    "        'SCHL',\n",
    "        'MAR',\n",
    "        'CIT',\n",
    "        'RELP',\n",
    "        'WKHP',\n",
    "        'PWGTP',\n",
    "        'SEX',\n",
    "        'RAC1P'\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,\n",
    "    group=['SEX', \"RAC1P\"],\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Bias Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the task 1 we should use data, which is not hot-encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "\n",
    "features_for_analysis, labels_for_analysis, groups_for_analysis = ACSIncomeNew.df_to_pandas(\n",
    "    acs_data, categories=categories)\n",
    "features_for_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(features, labels, groups, N=20000):\n",
    "\n",
    "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "        features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "    N = 20000  # Subsampling due to speed\n",
    "\n",
    "    X_train = X_train[:N]\n",
    "    y_train = y_train[:N]\n",
    "    group_train = group_train[:N]\n",
    "    X_test = X_test[:N]\n",
    "    y_test = y_test[:N]\n",
    "    group_test = group_test[:N]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, group_train, group_test\n",
    "\n",
    "X_train_for_analysis, X_test_for_analysis, y_train_for_analysis, y_test_for_analysis, group_train_for_analysis, group_test_for_analysis = split_dataset(\n",
    "    features_for_analysis, labels_for_analysis, groups_for_analysis, N=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_analyses = pd.DataFrame({'AGEP': X_train_for_analysis[:, 0], 'COW': X_train_for_analysis[:, 1], \n",
    "                       'SCHL': X_train_for_analysis[:, 2], 'MAR': X_train_for_analysis[:, 3], 'CIT': X_train_for_analysis[:, 4], 'RELP': X_train_for_analysis[:, 5],\n",
    "                       'WKHP': X_train_for_analysis[:, 6], 'PWGTP': X_train_for_analysis[:, 7], 'SEX': X_train_for_analysis[:, 8],\n",
    "                        'RAC1P': X_train_for_analysis[:, 9]})\n",
    "dataset_for_analyses['PINCP'] = y_train_for_analysis.tolist()\n",
    "dataset_for_analyses.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.: Data Collection and Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Discuss sources of bias in the dataset and in the selected features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(dataset_for_analyses, x=\"AGEP\", bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(dataset_for_analyses, x=\"WKHP\", bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(dataset_for_analyses, x=\"PWGTP\", bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for col in dataset_for_analyses[[\"COW\", \"SCHL\", \"MAR\", \"CIT\", \"RELP\", \"SEX\", \"RAC1P\", \"PINCP\"]]:\n",
    "    plt.figure().set_figwidth(15)\n",
    "    dataset_for_analyses[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Value counts for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "#from 1994"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.: Proxies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Look at feature distributions if you split them by SEX groups (for Males and Females, separately).\n",
    "Do you see any potential sources of bias? Provide arguments.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['AGEP', 'WKHP', 'PWGTP']:\n",
    "    #sns.catplot(data=dataset_for_analyses, y=col, hue=\"SEX\", kind=\"count\",palette=\"pastel\", edgecolor=\".6\", orient =\"h\", height=8.27, aspect=11.7/8.27)\n",
    "    sns.displot(dataset_for_analyses, x=col, hue=\"SEX\", kind=\"kde\", fill=True)\n",
    "    plt.title(f\"Value counts for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['RAC1P', 'MAR', 'SCHL', 'CIT', 'COW', 'PINCP']:\n",
    "    sns.catplot(data=dataset_for_analyses, y=col, hue=\"SEX\", kind=\"count\",palette=\"pastel\", edgecolor=\".6\", orient =\"h\", height=8.27, aspect=11.7/8.27)\n",
    "    plt.title(f\"Value counts for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for RAC1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['AGEP', 'WKHP', 'PWGTP']:\n",
    "    sns.displot(dataset_for_analyses, x=col, hue=\"RAC1P\", kind=\"kde\", fill=True)\n",
    "    plt.title(f\"Value counts for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['SEX', 'MAR', 'SCHL', 'CIT', 'COW', 'PINCP']:\n",
    "    sns.catplot(data=dataset_for_analyses, y=col, hue=\"RAC1P\", kind=\"count\",palette=\"pastel\", edgecolor=\".6\", orient =\"h\", height=8.27, aspect=11.7/8.27)\n",
    "    plt.title(f\"Value counts for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Look at the correlations between SEX and other variables.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. When discussing correlations, do not forget to use the correct metric (e.g. continuous-categorical\n",
    "features etc.). You can use dython.nominal.associations and seaborn.heatmap.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CramerV for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label = preprocessing.LabelEncoder()\n",
    "label_encoded = pd.DataFrame() \n",
    "\n",
    "for i in dataset_for_analyses[['RAC1P', 'MAR', 'SEX', 'SCHL', 'CIT', 'COW', 'PINCP']]:\n",
    "  label_encoded[i]=label.fit_transform(dataset_for_analyses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def cramers_V(var1,var2) :\n",
    "  crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n",
    "  stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n",
    "  obs = np.sum(crosstab) # Number of observations\n",
    "  mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n",
    "  return (stat/(obs*mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows= []\n",
    "\n",
    "for var1 in label_encoded:\n",
    "  col = []\n",
    "  for var2 in label_encoded :\n",
    "    cramers =cramers_V(label_encoded[var1], label_encoded[var2]) # Cramer's V test\n",
    "    col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n",
    "  rows.append(col)\n",
    "  \n",
    "cramers_results = np.array(rows)\n",
    "cr = pd.DataFrame(cramers_results, columns = label_encoded.columns, index =label_encoded.columns)\n",
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(cr, vmin=0, vmax=0.2, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import association\n",
    "import pandas as pd\n",
    "#['RAC1P', 'MAR', 'SEX', 'SCHL', 'CIT', 'COW', 'PINCP']]\n",
    "cont_table = pd.crosstab(index=[dataset_for_analyses['SEX']], columns=[dataset_for_analyses['RAC1P'], dataset_for_analyses['MAR'],\n",
    "                                                                       dataset_for_analyses['SCHL'], dataset_for_analyses['CIT'],\n",
    "                                                                        dataset_for_analyses['COW'], dataset_for_analyses['PINCP']]).values\n",
    "cramer_v = association(cont_table)\n",
    "# Calculate Cramer's V statistic\n",
    "cramer_v = association(cont_table)\n",
    "\n",
    "print(\"Cramer's V statistic:\", cramer_v)\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['RAC1P']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['MAR']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['SCHL']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['CIT']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['COW']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = pd.crosstab(dataset_for_analyses['SEX'], dataset_for_analyses['PINCP']).values\n",
    "cramer_v = association(cont_table)\n",
    "print(\"Cramer's V statistic:\", cramer_v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point-biserial correlation coefficient for categorical variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "point_label_encoded = pd.DataFrame() \n",
    "point = dataset_for_analyses[['SEX', 'AGEP','WKHP', 'PWGTP']]\n",
    "point\n",
    "for i in point.columns:\n",
    "  point_label_encoded[i]=label.fit_transform(dataset_for_analyses[i])\n",
    "print(stats.pointbiserialr(point_label_encoded['AGEP'], point_label_encoded['SEX']))\n",
    "print(stats.pointbiserialr(point_label_encoded['WKHP'], point_label_encoded['SEX']))\n",
    "print(stats.pointbiserialr(point_label_encoded['PWGTP'], point_label_encoded['SEX']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded['SEX'], point_label_encoded['AGEP'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['SEX', 'AGEP'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded['SEX'], point_label_encoded['WKHP'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['SEX', 'WKHP'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded['SEX'], point_label_encoded['PWGTP'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['SEX', 'PWGTP'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for RAC1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "point_label_encoded2 = pd.DataFrame() \n",
    "point = dataset_for_analyses[['RAC1P', 'AGEP','WKHP', 'PWGTP']]\n",
    "point\n",
    "for i in point.columns:\n",
    "  point_label_encoded2[i]=label.fit_transform(dataset_for_analyses[i])\n",
    "print(stats.pointbiserialr(point_label_encoded2['AGEP'], point_label_encoded2['RAC1P']))\n",
    "print(stats.pointbiserialr(point_label_encoded2['WKHP'], point_label_encoded2['RAC1P']))\n",
    "print(stats.pointbiserialr(point_label_encoded2['PWGTP'], point_label_encoded2['RAC1P']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded2['AGEP'], point_label_encoded2['RAC1P'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['AGEP', 'RAC1P'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded2['WKHP'], point_label_encoded2['RAC1P'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['WKHP', 'RAC1P'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pointbiserialr(point_label_encoded2['PWGTP'], point_label_encoded2['RAC1P'])\n",
    "\n",
    "plt.bar([0, 1], [r, 0], tick_label=['PWGTP', 'RAC1P'])\n",
    "plt.ylim([-1, 1])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Point-Biserial Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(dataset_for_analyses)\n",
    "g.map_upper(sns.histplot)\n",
    "g.map_lower(sns.kdeplot, fill=True)\n",
    "g.map_diag(sns.histplot, kde=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Model & Data Debiasing\n",
    "Now we are going to train a model to predict the income of a person based on the attributes we have\n",
    "at hand. We want to have a model with the high predictive performance, but we also want to make\n",
    "sure that our model does not discriminate against any protected groups."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.: Data\n",
    "1. Convert categorical to one-hot encoding.\n",
    "2. Remove redundant categorical columns (as you have done in Lecture 6).\n",
    "3. Remove protected attributes from the data (keep it aside).\n",
    "4. Split data into Training and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "\n",
    "# 1- Convert categroical to one-hot encoding\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(\n",
    "    acs_data, categories=categories, dummies=True)\n",
    "# groups now contain information about SEX and RAC1P\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Drop the \"redundant\" columns\n",
    "features = features.drop([\"RELP_Unmarried partner\",\n",
    "                          \"CIT_U.S. citizen by naturalization\",\n",
    "                          \"SEX_Male\",\n",
    "                          \"SCHL_1 or more years of college credit, no degree\",\n",
    "                          \"MAR_Divorced\",\n",
    "                          \"RELP_Adopted son or daughter\",\n",
    "                          'COW_Working without pay in family business or farm',\n",
    "                          \"RAC1P_White alone\"], axis=1)\n",
    "\n",
    "print(\"Columns with the protected features:\")\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"RAC1P\" in f) or (\"SEX\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Drop protected features from features\n",
    "protected = [\n",
    "    \"SEX_Female\",\n",
    "    \"RAC1P_Black or African American alone\"\n",
    "]\n",
    "if protected[0] in features.columns.to_list() or protected[1] in features.columns.to_list():\n",
    "    features = features.drop(protected, axis=1)\n",
    "else:\n",
    "    print(\"No protected features present already\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features in a different dataframe (used for PCA)\n",
    "def scale_columns(data, columns=[\"AGEP\", \"WKHP\", \"PWGTP\"], scaler=MinMaxScaler()):\n",
    "\n",
    "    for column in columns:\n",
    "        data[column] = scaler.fit_transform(\n",
    "            np.array(data[column]).reshape(-1, 1))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "features_normalized = features.copy()\n",
    "features_normalized = scale_columns(features_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_normalized.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- . Split data into Training and Test sets (used for PCA)\n",
    "def split_dataset(features, labels, groups, N=20000):\n",
    "\n",
    "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "        features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "    N = 20000  # Subsampling due to speed\n",
    "\n",
    "    X_train = X_train[:N]\n",
    "    y_train = y_train[:N]\n",
    "    group_train = group_train[:N]\n",
    "    X_test = X_test[:N]\n",
    "    y_test = y_test[:N]\n",
    "    group_test = group_test[:N]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, group_train, group_test\n",
    "\n",
    "\n",
    "# Split already normalized dataset\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = split_dataset(\n",
    "    features_normalized, labels, groups, N=20000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2 - Baseline Model\n",
    "Use the following arguments in the `opt.fmin_funct`: `xtol=1e-4, ftol=1e-4,  maxfun=1000`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1: Build your own implementation of the Logistic Regression with L2-penalty (aka Ridge Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions for custom implementation\n",
    "\n",
    "def sigmoid(beta_x, eps=1e-10):\n",
    "    \"\"\"\n",
    "    This is logistic regression\n",
    "    f = 1/(1+exp(-beta^T * x))  \n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-beta_x + eps))\n",
    "\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_preds are probabilities\n",
    "    eps: epsilon for stability\n",
    "    \"\"\"\n",
    "    summand_1 = y_true * np.log(y_pred + eps)\n",
    "    summand_2 = (1-y_true) * np.log(1 - y_pred + eps)\n",
    "\n",
    "    return -np.mean(summand_1 + summand_2)\n",
    "\n",
    "\n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    return sum(beta**2)\n",
    "\n",
    "\n",
    "def fair_loss(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Group fairness Loss\n",
    "\n",
    "    y_pred: sigmoid(βTx)\n",
    "    y: prediction in 0 or 1\n",
    "    groups: dataframe with column for each protected variable\n",
    "    \"\"\"\n",
    "\n",
    "    cum_result = 0\n",
    "\n",
    "    # compute once per protected variable\n",
    "    for p_group in groups.columns.to_list():\n",
    "\n",
    "        group = groups[p_group].to_numpy()\n",
    "        \n",
    "\n",
    "        y_g1 = y[group == 1]\n",
    "        y_pred_g1 = y_pred[group == 1]\n",
    "        y_g2 = y[group == 2]\n",
    "        y_pred_g2 = y_pred[group == 2]\n",
    "\n",
    "        n1 = np.sum(group == 1)\n",
    "        n2 = np.sum(group == 2)\n",
    "\n",
    "        # INDIVIDUAL CONSTRAINT\n",
    "        \n",
    "        # all pairwise equalities between y_g1 and y_g2\n",
    "        distance = np.equal.outer(y_g1, y_g2).astype(int)\n",
    "        # all pairwise differences between y_pred_g1 and y_pred_g2\n",
    "        diff = np.subtract.outer(y_pred_g1, y_pred_g2)      \n",
    "        diff_squared = diff ** 2 # square the differenes\n",
    "        cost = np.sum(distance * diff_squared)  # multiply both\n",
    "        result = (cost/(n1*n2))  # result for this protected variable\n",
    "        \n",
    "        cum_result += result  # combined result for all proetected variables\n",
    "        \n",
    "        # print(f'los for {p_group}: {result}')\n",
    "\n",
    "    return cum_result\n",
    "\n",
    "\n",
    "def fair_loss_Germans_modified(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Group fairness Loss - Germans implementation\n",
    "    note: using this one bcs faster\n",
    "    \"\"\"\n",
    "\n",
    "    cum_result = 0\n",
    "\n",
    "    # compute once per protected variable\n",
    "    for p_group in groups.columns.to_list():\n",
    "\n",
    "        group = groups[p_group].to_numpy()\n",
    "        \n",
    "        n = y.shape[0]\n",
    "        n1 = np.sum(group == 1)\n",
    "        n2 = np.sum(group == 2)\n",
    "        cost = 0\n",
    "\n",
    "        equal_pairs = np.argwhere(y[np.newaxis, :] == y[:, np.newaxis])\n",
    "        equal_pairs = equal_pairs[np.where(equal_pairs[:,0] != equal_pairs[:,1])] \n",
    "        diff_groups = (group[equal_pairs[:,0]] != group[equal_pairs[:,1]]).astype(int) \n",
    "        cost = diff_groups.dot((y_pred[equal_pairs[:,0]] - y_pred[equal_pairs[:,1]])**2) ## \n",
    "        cost = cost/(n1*n2)\n",
    "        cum_result += cost\n",
    "\n",
    "    return cum_result\n",
    "\n",
    "def compute_gradient_withl2(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values. \n",
    "       You do not need to use groups and lambda (fmin_tnc expects same input as \n",
    "       in func, that's why they are included here)\n",
    "\n",
    "       Note: we do not include fairness constrint here\n",
    "       \"\"\"\n",
    "\n",
    "    grad = np.zeros(beta.shape)\n",
    "    y_pred = sigmoid(X.dot(beta))\n",
    "    diff = y_pred - y\n",
    "    m = beta.shape\n",
    "\n",
    "    # Loop over features and correponding weights and calcualte gradient for each\n",
    "    for j in range(len(grad)):\n",
    "        if j == 0:\n",
    "            # we do not want to regularize the intercept\n",
    "            grad[j] = np.dot(diff, X[:, j])/m\n",
    "        else:\n",
    "            grad[j] = np.dot(diff, X[:, j])/m + 2*beta[j]*_gamma\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def compute_cost_withl2(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "\n",
    "    # Logistic function prediction and real value, all in numeric\n",
    "    y_pred = sigmoid(X.dot(beta))\n",
    "    y_real = np.array(y).astype(int)\n",
    "\n",
    "    # Calculate each loss and add up\n",
    "    loss_log = logistic_loss(y_real, y_pred, eps=1e-10)\n",
    "    loss_l2 = _gamma * l2_loss(beta[1:])  # l2 loss does not consider intercept\n",
    "\n",
    "    loss = loss_log + \\\n",
    "        loss_l2\n",
    "        \n",
    "    # print('log',round(loss_log, 4), 'l2', round(loss_l2,4), 'TOTAL:', round(loss,4))\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_cost_withl2_withfairness(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "\n",
    "    # Logistic function prediction and real value, all in numeric\n",
    "    y_pred = sigmoid(X.dot(beta))\n",
    "    y_real = np.array(y).astype(int)\n",
    "\n",
    "    # Calculate each loss and add up\n",
    "    loss_log = logistic_loss(y_real, y_pred, eps=1e-10)\n",
    "    # loss_fair = _lambda * fair_loss(y_real, y_pred, groups)\n",
    "    loss_fair = _lambda * fair_loss(y_real, y_pred, groups)\n",
    "    loss_l2 = _gamma * l2_loss(beta[1:])  # l2 loss does not consider intercept\n",
    "\n",
    "    loss =  loss_log + \\\n",
    "        loss_l2 + \\\n",
    "            loss_fair\n",
    "\n",
    "    # print('log',round(loss_log, 4),'fair', round(loss_fair,7),'l2', round(loss_l2,4), 'TOTAL:', round(loss,4))\n",
    "    # print('log',round(loss_log, 4), 'l2', round(loss_l2,4), 'TOTAL:', round(loss,4))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fit_logistic_l2(X, y, groups, lambda_used, gamma_used, include_fairness=False, aprox_grad=False, verbose=False):\n",
    "    '''\n",
    "    Function that finds optimal weights for given input\n",
    "\n",
    "    X: numpy array with features\n",
    "    y: numpy array with labels\n",
    "    groups: pandas dataframe with protected groups binary variables\n",
    "\n",
    "    '''\n",
    "\n",
    "    betas = np.random.rand(X.shape[1])\n",
    "\n",
    "    if include_fairness:\n",
    "        if verbose:\n",
    "            print(f\"Including fairness in cost\")\n",
    "        cost_func = compute_cost_withl2_withfairness\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Not including fairness in cost\")\n",
    "        cost_func = compute_cost_withl2\n",
    "        \n",
    "    if aprox_grad:\n",
    "        if verbose:\n",
    "            print('Using approx_grad=True')\n",
    "        new_weights, q, code = opt.fmin_tnc(func=cost_func, x0=betas, messages=0,  xtol=1e-4, ftol=1e-4, approx_grad=True, maxfun=100,\n",
    "                                            args=(X, y, groups, lambda_used, gamma_used))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('Using custom function for gradient')\n",
    "        new_weights, q, code = opt.fmin_tnc(func=cost_func, x0=betas, messages=0,  xtol=1e-4, ftol=1e-4, fprime=compute_gradient_withl2, maxfun=100,\n",
    "                                            args=(X, y, groups, lambda_used, gamma_used), )\n",
    "    return new_weights, q, code \n",
    "\n",
    "\n",
    "def find_optimalparameters_withxval(X, y, groups, lambda_values, gamma_values, n_folds=5, include_fairness=False, verbose=False):\n",
    "    '''\n",
    "    Finds optimal combination of lambda and gamma in terms of accuracy using \n",
    "    cross-validation\n",
    "    '''\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    all_validation_scores = []\n",
    "    l, g = np.meshgrid(lambda_values, gamma_values)\n",
    "    all_combinations = np.column_stack((l.ravel(), g.ravel()))\n",
    "\n",
    "    # Calculate score for each possible value and store\n",
    "    for combination in all_combinations: \n",
    "        fair_penalty = combination[0]\n",
    "        l2_penalty = combination[1]\n",
    "        print(f\"Lambda: {fair_penalty}, Gamma: {l2_penalty}\")\n",
    "        \n",
    "        crossvalidation_scores = [] # here we save result for each crossvalidation run\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            # split the data into training and validation sets\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            groups_train, groups_val = groups.iloc[train_idx], groups.iloc[val_idx]\n",
    "\n",
    "            # normalize the data using the training set and apply the same transformation to the validation set\n",
    "            scaler.fit(X_train)\n",
    "            X_train_norm = scaler.transform(X_train)\n",
    "            X_val_norm = scaler.transform(X_val)\n",
    "\n",
    "            # Fit custom model\n",
    "            weights, _, _ = fit_logistic_l2(X_train_norm, y_train, groups_train,\n",
    "                                    lambda_used = fair_penalty, gamma_used = l2_penalty, include_fairness=include_fairness, verbose=verbose)\n",
    "            prob = sigmoid(X_val_norm.dot(weights))\n",
    "            y_pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "            score = np.mean(y_pred == y_val)\n",
    "\n",
    "            crossvalidation_scores.append(score)\n",
    "            \n",
    "        mean_score = np.mean(crossvalidation_scores)\n",
    "        print(f\"---> mean score: {round(mean_score, 4)}\")\n",
    "\n",
    "        all_validation_scores.append(mean_score) # we save mean result for each parameter combination\n",
    "\n",
    "    optimal_fair_penalty, optimal_l2_penalty  = all_combinations[np.argmax(all_validation_scores)]\n",
    "    \n",
    "    return optimal_l2_penalty, optimal_fair_penalty, all_validation_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2: Use Cross-Validation to find the most optimal value for L2-penalty (you should implement it yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataframes\n",
    "sampled_rows = features.sample(n=20000, random_state=0).index\n",
    "features_sampled = features.loc[sampled_rows]\n",
    "groups_sampled = groups.loc[sampled_rows]\n",
    "labels_sampled = labels.loc[sampled_rows]\n",
    "\n",
    "# Add column to features for intercept\n",
    "features_sampled = features_sampled.insert(0, 'intercept', 1)\n",
    "\n",
    "# Define search space of gammas (penalty for l2 norm)\n",
    "gammas = np.logspace(-5, -2, num=10, base=1e1) # we search from 1e-5 to 1e-2\n",
    "lambdas = [0] # we don't care about fairness penalty now\n",
    "\n",
    "\n",
    "# 2- Perform crossvalidation and get best gamma. \n",
    "# note: calculation commented and just loading results\n",
    "\n",
    "# optimal_l2_penalty, optimal_fair_penalty, all_validation_scores = find_optimalparameters_withxval(\n",
    "#     features.values, labels.values.reshape(-1), groups, gamma_values = gammas, lambda_values=lambdas, n_folds=5)\n",
    "\n",
    "outpath = os.path.join(os.getcwd(), \"out\", \"task2\")\n",
    "file_path = os.path.join(outpath, 'optimal_gamma.pkl')\n",
    "\n",
    "# # # save results\n",
    "# if not os.path.exists(outpath):\n",
    "#     os.mkdir(outpath)\n",
    "# with open(file_path, 'wb') as f:\n",
    "#     pickle.dump([optimal_l2_penalty, optimal_fair_penalty, all_validation_scores ], f)\n",
    "\n",
    "# load results\n",
    "with open(file_path, 'rb') as f:\n",
    "    optimal_l2_penalty, optimal_fair_penalty, all_validation_scores = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimal lambda found using cross-validation: {optimal_l2_penalty}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 - Evaluate the overall performance of the final model on the Test Set (use an appropriate metrics) + report uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary funcitons for plotting and evlauating \n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate uncertainty\n",
    "    num_samples = len(y_test)\n",
    "    num_bootstraps = 1000\n",
    "    bootstrap_indices = np.random.randint(0, num_samples, (num_bootstraps, num_samples))\n",
    "    bootstrap_scores = np.zeros((num_bootstraps, 4))\n",
    "    \n",
    "    for i in range(num_bootstraps):\n",
    "        y_test_bootstrap = y_test[bootstrap_indices[i]]\n",
    "        y_pred_bootstrap = y_pred[bootstrap_indices[i]]\n",
    "        bootstrap_scores[i, 0] = accuracy_score(y_test_bootstrap, y_pred_bootstrap)\n",
    "        bootstrap_scores[i, 1] = precision_score(y_test_bootstrap, y_pred_bootstrap)\n",
    "        bootstrap_scores[i, 2] = recall_score(y_test_bootstrap, y_pred_bootstrap)\n",
    "        bootstrap_scores[i, 3] = f1_score(y_test_bootstrap, y_pred_bootstrap)\n",
    "    \n",
    "    # std for confidence intervals\n",
    "    accuracy_std = np.std(bootstrap_scores[:, 0])\n",
    "    precision_std = np.std(bootstrap_scores[:, 1])\n",
    "    recall_std = np.std(bootstrap_scores[:, 2])\n",
    "    f1_std = np.std(bootstrap_scores[:, 3])\n",
    "    \n",
    "    # Return metrics and uncertainty in dictionary\n",
    "    return {'accuracy': [accuracy,accuracy_std], 'precision': [precision,precision_std], 'recall': [recall, recall_std], 'f1': [f1, f1_std]}\n",
    "        \n",
    "def plot_metrics(metrics, ax, fig, savefig=False, plotfig=True, label=None, title='Model Performance', filename=None):\n",
    "    \n",
    "    # extract metrics and uncertainties\n",
    "    metric_names = list(metrics.keys())\n",
    "    metric_values = [m[0] for m in metrics.values()]\n",
    "    metric_uncertainties = [m[1] for m in metrics.values()]\n",
    "\n",
    "    # create bar plot with error bars\n",
    "    ax.errorbar(metric_names, metric_values, yerr=metric_uncertainties,label=label, \\\n",
    "                    fmt='o',capsize=4, alpha=0.7, markersize=3)\n",
    "\n",
    "\n",
    "    # add title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "    # set y-axis limits\n",
    "    ax.set_ylim([0.5, 1])\n",
    "\n",
    "    # add grid lines\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    if plotfig:\n",
    "        plt.show()\n",
    "    \n",
    "    if savefig:\n",
    "        outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "        if not os.path.exists(outpath):\n",
    "            os.mkdir(outpath)\n",
    "        fig.savefig(os.path.join(outpath, filename), dpi=200, bbox_inches='tight' )\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to our data and obtain prediction\n",
    "weights, q, code  = fit_logistic_l2(np.insert(X_train, 0, 1, axis=1), y_train, groups=None,\n",
    "                            gamma_used=optimal_l2_penalty,lambda_used=optimal_fair_penalty, include_fairness=False)\n",
    "prob = sigmoid(np.insert(X_test, 0, 1, axis=1).dot(weights))\n",
    "y_pred = (prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_model(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax = plot_metrics(evaluation_results, ax,fig, savefig=True, filename=\"Task2.2_part3_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plot_metrics(evaluation_results, ax,fig, savefig=True, filename=\"Task2.2_part3_plot.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 - Look at the fairness metric associated with each SEX and RAC1P group. Are there any discrepancies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate performance plot split by protected features' values\n",
    "# fig, axes = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "# for i, group_name in enumerate(group.columns.to_list()):\n",
    "    \n",
    "#     print(group_name)\n",
    "#     vals = group[group_name].unique().tolist()\n",
    "    \n",
    "#     for val in vals:\n",
    "        \n",
    "#         # Only test set changes, same weights used all the time\n",
    "#         X_test_temp = X_test[group_test[group_name] == val]\n",
    "#         y_test_temp  = y_test[group_test[group_name] == val]\n",
    "\n",
    "#         prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights))\n",
    "#         y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "#         evaluation_results_temp = evaluate_model( y_test_temp,y_pred_temp)\n",
    "\n",
    "#         axes[i] = plot_metrics(evaluation_results_temp, axes[i], fig, label=val, plotfig=False, savefig=False, title=f'Model performance by {group_name}')\n",
    "    \n",
    "#     axes[i].legend()\n",
    "\n",
    "# outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "# if not os.path.exists(outpath):\n",
    "#     os.mkdir(outpath)\n",
    "# fig.savefig(os.path.join(outpath, \"Task2.2_part4_plot.png\"), dpi=200, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "def get_fnr_and_fpr(y_true, y_pred):\n",
    "\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label == 1 and pred_label == 1:\n",
    "            true_positive += 1\n",
    "        elif true_label == 1 and pred_label == 0:\n",
    "            false_negative += 1\n",
    "        elif true_label == 0 and pred_label == 0:\n",
    "            true_negative += 1\n",
    "        elif true_label == 0 and pred_label == 1:\n",
    "            false_positive += 1\n",
    "    \n",
    "    if (true_positive + false_negative) == 0:\n",
    "        fnr = 0\n",
    "    else:\n",
    "        fnr = false_negative / (true_positive + false_negative)\n",
    "        \n",
    "    if (true_negative + false_positive) == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = false_positive / (true_negative + false_positive)\n",
    "        \n",
    "    return fnr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnr_all = {}\n",
    "fpr_all = {}\n",
    "\n",
    "for i, group_name in enumerate(group.columns.to_list()):\n",
    "    \n",
    "    print(group_name)\n",
    "    vals = group[group_name].unique().tolist()\n",
    "    \n",
    "    for val in vals:\n",
    "        \n",
    "        # Only test set changes, same weights used all the time\n",
    "        X_test_temp = X_test[group_test[group_name] == val]\n",
    "        y_test_temp  = y_test[group_test[group_name] == val]\n",
    "\n",
    "        prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights))\n",
    "        y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "        fnr_temp, fpr_temp  = get_fnr_and_fpr(y_test_temp,y_pred_temp)\n",
    "\n",
    "        fnr_all[(group_name, val)] = round(fnr_temp, ndigits=5)\n",
    "        fpr_all[(group_name, val)] = round(fpr_temp, ndigits=5)\n",
    "\n",
    "fairness_metrics = pd.DataFrame(data=[fnr_all, fpr_all], index=['FNR', 'FPR']).reset_index(names='metric')\n",
    "\n",
    "# Save\n",
    "outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "if not os.path.exists(outpath):\n",
    "    os.mkdir(outpath)\n",
    "fairness_metrics.to_csv(os.path.join(outpath, 'task2.2_part4.csv'), index=False)\n",
    "fairness_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "Use the following arguments in the `opt.fmin_funct`: ` xtol=1e-3, ftol=1e-3, approx_grad=True, maxfun=1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split already normalized into even smaller pieces, since this part is very expensive computationally\n",
    "X_train_small, X_test_small, y_train_small, y_test_small, group_train_small, group_test_small = split_dataset(\n",
    "    features_normalized, labels, groups, N=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Add Individual Fairness Penalty to your baseline model (refer to Lecture 5 Exercises)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Plot Pareto Curve by varying λ = [1e − 3, 5e − 3, 1e − 2, 5e − 2, 0.1, 1], evaluate the performance of the model (using your favourite metric). Plot a curve for each group of protected attributes (i.e. 4 curves). What happens as we increase the penalty? Is there a point where all groups get similar performance metric values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: calculation has been commented and results are loaded, since it takes too long (one full night)\n",
    "\n",
    "lambdas = np.array([1e-3, 5e-3, 1e-2, 5e-2, 0.1, 1])\n",
    "\n",
    "outpath = os.path.join(os.getcwd(), \"out\", \"task2\")\n",
    "\n",
    "if not os.path.exists(outpath):\n",
    "   os.mkdir(outpath)\n",
    "\n",
    "# calculate results\n",
    "# fairness_results  = {}\n",
    "# for lambda_ in lambdas:\n",
    "#    print(lambda_)\n",
    "#    file_path = os.path.join(outpath, f'results_lambda_{lambda_}.pkl')\n",
    "#    weights_fair, q_fair, code_fair = fit_logistic_l2(np.insert(X_train_small, 0, 1, axis=1), y_train_small, groups=group_train_small,\n",
    "#                             gamma_used=optimal_l2_penalty, lambda_used=lambda_, include_fairness=True, aprox_grad=True, verbose=False)\n",
    "#    with open(file_path, 'wb') as f:\n",
    "#       pickle.dump([weights_fair, q_fair, code_fair], f)\n",
    "\n",
    "#    fairness_results[lambda_] = [weights_fair, q_fair, code_fair]\n",
    "   \n",
    "# with open(os.path.join(outpath, f'results_ALL_lambdas.pkl'), 'wb') as f:\n",
    "#       pickle.dump(fairness_results, f)\n",
    "      \n",
    "# load results\n",
    "with open(os.path.join(outpath, f'results_ALL_lambdas.pkl'), 'rb') as f:\n",
    "   fairness_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance data for each group and lambda values\n",
    "performance = {}\n",
    "    \n",
    "for i, group_name in enumerate(groups.columns.to_list()):\n",
    "\n",
    "    vals = group[group_name].unique().tolist()\n",
    "\n",
    "    # Fill data per group\n",
    "    for val in vals:\n",
    "            \n",
    "        performance[(group_name, val)] = []\n",
    "        print(group_name, val)\n",
    " \n",
    "        for lambda_ in lambdas:\n",
    "    \n",
    "            weights_temp = fairness_results[lambda_][0]\n",
    "            \n",
    "            X_test_temp = X_test[group_test[group_name] == val]\n",
    "            y_test_temp  = y_test[group_test[group_name] == val]\n",
    "\n",
    "            prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights_temp))\n",
    "            y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "            accuracy = np.mean((y_pred_temp >= 0.5).astype(int) == y_test_temp)\n",
    "            performance[(group_name, val)].append(accuracy)\n",
    "\n",
    "    # Fill combined data\n",
    "    \n",
    "    performance[(group_name, 'all')] = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        \n",
    "        # combined performance    \n",
    "        weights_temp = fairness_results[lambda_][0]\n",
    "        \n",
    "        X_test_temp = X_test\n",
    "        y_test_temp  = y_test\n",
    "\n",
    "        prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights_temp))\n",
    "        y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = np.mean((y_pred_temp >= 0.5).astype(int) == y_test_temp)\n",
    "        performance[(group_name, 'all')].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for Pareto curves using previous data\n",
    "fig, axes = plt.subplots(1,2, figsize=(16,6), sharey=True, sharex=True)\n",
    "\n",
    "for i, group_name in enumerate(group.columns.to_list()):\n",
    "    \n",
    "    vals = group[group_name].unique().tolist()\n",
    "            \n",
    "    for val in vals:    \n",
    "        # add title and labels\n",
    "        axes[i].set_title(f\"Pareto Curve for {group_name}\")\n",
    "        axes[i].set_xlabel(\"Fairness Lambda\")\n",
    "        axes[i].set_ylabel(\"Accuracy\")\n",
    "\n",
    "        # add grid lines\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[i].plot(lambdas, performance[(group_name, val)], label=val, marker='o')\n",
    "    \n",
    "    axes[i].plot(lambdas, performance[(group_name, 'all')], label='all', marker='o')\n",
    "    axes[i].legend()\n",
    "\n",
    "outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "if not os.path.exists(outpath):\n",
    "    os.mkdir(outpath)\n",
    "fig.savefig(os.path.join(outpath, \"Task2.3_part2_plot.png\"), dpi=200, bbox_inches='tight' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 - Set λ = 0.1 and evaluate the overall performance of the final model on the Test Set (report uncertainty). Use the same metric as you used in Task 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_fair, q_fair, code_fair = fairness_results[0.1] # get only result for 0.1\n",
    "prob_fair = sigmoid(np.insert(X_test, 0, 1, axis=1).dot(weights_fair))\n",
    "y_pred_fair = (prob_fair >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "evaluation_results_fair = evaluate_model(y_test, y_pred_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plot_metrics(evaluation_results_fair, ax,fig, savefig=True, filename=\"Task2.3_part3_plot.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 - Set λ = 0.1 and look at the fairness metric associated with each SEX and RAC1P group. What do you see (compare results to the baseline model)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate performance plot split by protected features' values\n",
    "# fig, axes = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "# for i, group_name in enumerate(group.columns.to_list()):\n",
    "    \n",
    "#     print(group_name)\n",
    "#     vals = group[group_name].unique().tolist()\n",
    "    \n",
    "#     for val in vals:\n",
    "        \n",
    "#         # Only test set changes, same weights used all the time\n",
    "#         X_test_temp = X_test[group_test[group_name] == val]\n",
    "#         y_test_temp  = y_test[group_test[group_name] == val]\n",
    "\n",
    "#         prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights_fair))\n",
    "#         y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "#         evaluation_results_temp = evaluate_model( y_test_temp,y_pred_temp)\n",
    "\n",
    "#         axes[i] = plot_metrics(evaluation_results_temp, axes[i], fig, label=val, plotfig=False, savefig=False, title=f'Model performance by {group_name}')\n",
    "    \n",
    "#     axes[i].legend()\n",
    "\n",
    "# outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "# if not os.path.exists(outpath):\n",
    "#     os.mkdir(outpath)\n",
    "# fig.savefig(os.path.join(outpath, \"Task2.3_part4_plot.png\"), dpi=200, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnr_all = {}\n",
    "fpr_all = {}\n",
    "\n",
    "for i, group_name in enumerate(group.columns.to_list()):\n",
    "    \n",
    "    print(group_name)\n",
    "    vals = group[group_name].unique().tolist()\n",
    "    \n",
    "    for val in vals:\n",
    "        \n",
    "        # Only test set changes, same weights used all the time\n",
    "        X_test_temp = X_test[group_test[group_name] == val]\n",
    "        y_test_temp  = y_test[group_test[group_name] == val]\n",
    "\n",
    "        prob_temp = sigmoid(np.insert(X_test_temp, 0, 1, axis=1).dot(weights_fair))\n",
    "        y_pred_temp = (prob_temp >= 0.5).astype(int)\n",
    "\n",
    "        fnr_temp, fpr_temp  = get_fnr_and_fpr(y_test_temp,y_pred_temp)\n",
    "\n",
    "        fnr_all[(group_name, val)] = round(fnr_temp, ndigits=5)\n",
    "        fpr_all[(group_name, val)] = round(fpr_temp, ndigits=5)\n",
    "\n",
    "fairness_metrics = pd.DataFrame(data=[fnr_all, fpr_all], index=['FNR', 'FPR']).reset_index(names='metric')\n",
    "\n",
    "# Save\n",
    "outpath = os.path.join(os.getcwd(), \"plots\", \"task2\")\n",
    "if not os.path.exists(outpath):\n",
    "    os.mkdir(outpath)\n",
    "fairness_metrics.to_csv(os.path.join(outpath, 'task2.3_part4.csv'), index=False)\n",
    "fairness_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4.: Fair PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected columns already dropped in features_normalized so no need to substract 4 for PCA column count\n",
    "n_components = len(features_normalized.columns)\n",
    "print(n_components)\n",
    "\n",
    "pca = PCA(n_components=n_components)  # create a PCA object\n",
    "X = pca.fit_transform(features_normalized)  # do the math\n",
    "\n",
    "pca_columns = []\n",
    "for i in range(n_components):\n",
    "    pca_columns.append(f\"PC{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_no_protected = pd.DataFrame(X, columns=pca_columns)\n",
    "pca_no_protected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1_merged = pd.concat([pca_no_protected, groups], axis=1)\n",
    "task_1_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "n_features = X_train.shape[1]\n",
    "alpha = 0.05  # significance level\n",
    "# Bonferroni correction for multiple testings\n",
    "corrected_alpha = alpha / (n_features**2/2)\n",
    "\n",
    "##############################\n",
    "# Your code here\n",
    "##############################\n",
    "\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = float(round(pearsonr(tmp[r], tmp[c])[1], 4))\n",
    "    return pvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = calculate_pvalues(task_1_merged)\n",
    "p_values = p_values.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_filtered_relevant(df, p_values, corrected_alpha, title):\n",
    "    sns.heatmap(df, annot=False, cmap='rocket', linewidths=0.5,\n",
    "                linecolor='black', mask=p_values > corrected_alpha)\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.tick_params(axis='both', which='both', labelsize=12)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_filtered_relevant(task_1_merged.corr(method='pearson')[\n",
    "                          ['SEX', 'RAC1P']], p_values[['SEX', 'RAC1P']], corrected_alpha, 'Pearson Correlation')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = len(X_test[0])\n",
    "print(n_components)\n",
    "\n",
    "pca = PCA(n_components=n_components)  # create a PCA object\n",
    "X_test_pca = pca.fit_transform(X_test)  # do the math\n",
    "\n",
    "# project back\n",
    "X_test_pca_proj_back = pca.inverse_transform(X_test_pca)\n",
    "rec_errors = X_test - X_test_pca_proj_back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_errors_pd = pd.DataFrame(rec_errors, columns=features_normalized.columns)\n",
    "rec_errors_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2_merged = pd.concat(\n",
    "    [rec_errors_pd.reset_index(), group_test.reset_index()], axis=1)\n",
    "task_2_merged = task_2_merged.drop([\"index\"], axis=1)\n",
    "task_2_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_rec_error = task_2_merged.loc[task_2_merged[\"SEX\"] == 1].mean().mean()\n",
    "group2_rec_error = task_2_merged.loc[task_2_merged[\"SEX\"] == 2].mean().mean()\n",
    "group3_rec_error = task_2_merged.loc[task_2_merged[\"RAC1P\"] == 1].mean().mean()\n",
    "group4_rec_error = task_2_merged.loc[task_2_merged[\"RAC1P\"] == 2].mean().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Group 1 reconstruction error mean: {group1_rec_error}\")\n",
    "print(f\"Group 2 reconstruction error mean: {group2_rec_error}\")\n",
    "print(f\"Group 3 reconstruction error mean: {group3_rec_error}\")\n",
    "print(f\"Group 4 reconstruction error mean: {group4_rec_error}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import null_space\n",
    "from scipy.linalg import eig\n",
    "\n",
    "\n",
    "def fair_pca(X, Z):\n",
    "    SEX_minus_mean = Z[\"SEX\"] - Z[\"SEX\"].mean()\n",
    "    RAC1P_minus_mean = Z[\"RAC1P\"] - Z[\"RAC1P\"].mean()\n",
    "    _Z = Z.copy()\n",
    "    _Z[\"SEX\"] = SEX_minus_mean\n",
    "    _Z[\"RAC1P\"] = RAC1P_minus_mean\n",
    "\n",
    "    R = null_space(_Z.to_numpy().T.dot(X))\n",
    "\n",
    "    rtx = R.T.dot(X.T)\n",
    "    rtxxt = rtx.dot(X)\n",
    "    rtxxtr = rtxxt.dot(R)\n",
    "\n",
    "    eigenvectors, eigenvalues = eig(rtxxtr)\n",
    "    L = eigenvalues\n",
    "    U = R.dot(L)\n",
    "    X_prim = X.dot(U)\n",
    "\n",
    "    return X_prim, U, _Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prim, U, _Z = fair_pca(X_train, group_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_pca_columns = []\n",
    "for i in range(len(X_prim[0])):\n",
    "    fair_pca_columns.append(f\"F_PC{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_pca_no_protected = pd.DataFrame(X_prim, columns=fair_pca_columns)\n",
    "fair_pca_no_protected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_3_merged = pd.concat(\n",
    "    [fair_pca_no_protected.reset_index(), _Z.reset_index()], axis=1)\n",
    "task_3_merged = task_3_merged.drop([\"index\"], axis=1)\n",
    "task_3_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = calculate_pvalues(task_3_merged)\n",
    "p_values = p_values.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_filtered_relevant(task_3_merged.corr(method='pearson')[\n",
    "                          ['SEX', 'RAC1P']], p_values[['SEX', 'RAC1P']], corrected_alpha, 'Pearson Correlation')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prim_pca, U, _Z = fair_pca(X_test, group_test)\n",
    "X_prim_pca.shape\n",
    "# project back\n",
    "\n",
    "X_test_fair_pca_proj_back = np.dot(X_prim_pca, U.T)\n",
    "rec_errors = X_test - X_test_fair_pca_proj_back\n",
    "rec_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_errors_fair_pd = pd.DataFrame(\n",
    "    rec_errors, columns=features_normalized.columns)\n",
    "rec_errors_fair_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_4_merged = pd.concat(\n",
    "    [rec_errors_fair_pd.reset_index(), group_test.reset_index()], axis=1)\n",
    "task_4_merged = task_4_merged.drop([\"index\"], axis=1)\n",
    "task_4_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_rec_error = task_4_merged.loc[task_4_merged[\"SEX\"] == 1].mean().mean()\n",
    "group2_rec_error = task_4_merged.loc[task_4_merged[\"SEX\"] == 2].mean().mean()\n",
    "group3_rec_error = task_4_merged.loc[task_4_merged[\"RAC1P\"] == 1].mean().mean()\n",
    "group4_rec_error = task_4_merged.loc[task_4_merged[\"RAC1P\"] == 2].mean().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Group 1 reconstruction error mean: {group1_rec_error}\")\n",
    "print(f\"Group 2 reconstruction error mean: {group2_rec_error}\")\n",
    "print(f\"Group 3 reconstruction error mean: {group3_rec_error}\")\n",
    "print(f\"Group 4 reconstruction error mean: {group4_rec_error}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fair_pca, _, _ = fair_pca(X_test, group_test)\n",
    "X_test_fair_pca  # 52 fair PCA components, they suggested using only first 30\n",
    "# X_test_fair_pca is without protected groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
